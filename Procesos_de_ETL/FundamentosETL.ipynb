{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4787db7b",
   "metadata": {},
   "source": [
    "# *Fundamentos de ETL (Extract, Transform, Load)*\n",
    "\n",
    "### ¿Qué es ETL?\n",
    "\n",
    "ETL es un proceso utilizado en la gestión de datos que implica tres etapas principales:\n",
    "\n",
    "- Extracción (Extract): Conectamos con los datos, sin importar dónde se encuentren (bases de datos, archivos, APIs, etc).\n",
    "\n",
    "- Transformación (Transform): Limpiamos, organizamos y damos forma a los datos para que sean útiles y coherentes.\n",
    "\n",
    "- Carga (Load): Entregamos los datos procesados a su destino final, listos para ser analizados y generar insights poderosos.\n",
    "\n",
    "### ¿Por qué es importante ETL?\n",
    "\n",
    "ETL es la base de la inteligencia de negocio. Sin datos limpios y estructurados, no puedes tomar decisiones informadas.\n",
    "\n",
    "- Descubre patrones ocultos: ETL te permite encontrar relaciones y tendencias en los datos que de otra manera pasarían desapercibidas.\n",
    "\n",
    "- Optimiza tus procesos: Identifica cuellos de botella, ineficiencias y áreas de mejoras gracias al análisis de datos.\n",
    "\n",
    "\n",
    "### Para diseñar un proceso de ETL se deben considerar los siguientes puntos importantes:\n",
    "\n",
    "- Privacidad de Datos: es crucial garantizar el cumplimiento de las regulaciones de privacidad de datos, como la LGPD en México. La anonimización y el cifrado de datos son prácticas esenciales.\n",
    "\n",
    "- Calidad de los Datos: La calidad de los datos es fundamental para obtener información precisa. La limpieza, validación y enriquecimiento de los datos son pasos clave en el proceso de transformación.\n",
    "\n",
    "- Escalabilidad: A medida que las empresas crecen, sus necesidades de datos también lo hacen. Es importante diseñar soluciones ETL que puedan escalar para manejar mayores volúmenes de datos.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *Arquitectura de un pipeline ETL*\n",
    "\n",
    "Los siguientes son los atributos de una canalización de datos sólida:\n",
    "\n",
    "- Expectativas claramente definidas: Define claramente los objetivos y requisitos del pipeline ETL.\n",
    "\n",
    "- Aqrquitectura escalable: Diseña la arquitectura del pipeline para que pueda manejar el crecimiento futuro de los datos y las demandas de procesamiento.\n",
    "\n",
    "- Reproducible y clara: Asegúrate de que el pipeline pueda ser reproducido y comprendido fácilmente por otros miembros del equipo.\n",
    "\n",
    "### Preguntas clave para diseñar un pipeline ETL:\n",
    "\n",
    "*¿De donde vienen los datos?*\n",
    "  - Identifica las fuentes de datos (bases de datos, archivos, APIs, etc).\n",
    "  - ¿Cuáles son los tipos de datos (estructurados, no estructurados, semi-estructurados)?\n",
    "  - ¿Qué datos deben recopilarse?\n",
    "  - ¿Cuando los puedo obtener?\n",
    "\n",
    "*¿Qué transformación, limpieza, filtrado, agregación deben aplicarse?*\n",
    "  - ¿Existen inconsistencias en los datos?\n",
    "  - ¿Necesitas aplicar estandarización o normalización?\n",
    "  - ¿Cuál es el stack disponible para realizar las transformaciones?\n",
    "\n",
    "*¿Dónde deben entregarse los datos?*\n",
    "  - ¿Cúal es el formato de entrega?\n",
    "  - ¿Cúal es la frecuencia de actualización?\n",
    "  - ¿El usuario final puede acceder a todos los datos obtenidos?\n",
    "\n",
    "\n",
    "## Diseño de Pipelines ETL eficientes\n",
    "\n",
    "*Modularidad:*\n",
    "  - Dividir el proceso en estapas claras y manejables (extracción, transformación, carga) para facilitar el mantenimiento y la escalabilidad.\n",
    "  \n",
    "*Paralelización:*\n",
    "  - Identificar oportunidades para ejecutar tareas en paralelo y aprovechar al máximo los recursos computacionales disponibles.\n",
    "\n",
    "*Toleracia a fallos:*\n",
    "  - Implementar mecanismos para manejar errores y recuperarse de interrupciones sin perder datos ni comprometer la integridad del sistema.\n",
    "\n",
    "*Monitoreo y registro:*\n",
    "  - Establecer un sistema de monitoreo y registro para ratrear el progreso del pipeline, identificar cuellos de botella y solucionar problemas de manera proactiva.\n",
    "\n",
    "## Identificación de fuentes y destinos de datos\n",
    "\n",
    "Es fundamental comprender los requerimientos, la estructura y el formato de los datos en cada fuente y destino para garantizar una integración fluida y evitar sorpresas desagradables durante el proceso de ETL.\n",
    "\n",
    "*Fuentes de datos:*\n",
    "  - Accesibilidad de los datos.\n",
    "  - Dependencias de limite y frecuencia\n",
    "  - Generación de llaves de acceso y autenticación\n",
    "  - Caracteristicas particulares de la fuente de datos\n",
    "  - Documentación de conexiones y APIs\n",
    "\n",
    "*Destinos de datos:*\n",
    "  - Acceso a la base de datos o sistema de almacenamiento\n",
    "  - Recursos de la fuenre de destino\n",
    "  - Esquemas de desarrollo y estructura de datos\n",
    "  - Requisitos de seguridad, gobernanza y cumplimiento\n",
    "  - Acceso para los usuarios finales\n",
    "  - Documentación requerida\n",
    "  - Procesos de pase a producción\n",
    "\n",
    "## Librerías mas importantes y populares para ETL\n",
    "\n",
    "Aunque existen muchas librerías para realizar procesos ETL, las siguientes son algunas de las más populares y ampliamente utilizadas en la comunidad de ciencia de datos y desarrollo de software:\n",
    "\n",
    "*Pandas:* Ideal para manipulación de datos.\n",
    "  - Lectura y escritura de archivos (CSV, Excel, JSON)\n",
    "  - Limpieza y transformación de datos\n",
    "  - Análisis exploratorio de datos\n",
    "\n",
    "*Polars:* El bólido de la velocidad oara Big Data.\n",
    "  - Diseñado para aprovechar al máximo el hardware moderno\n",
    "  - Ideal para trabajar con grandes volúmenes de datos\n",
    "  - Sintaxis similar a Pandas, pero con un rendimiento superior\n",
    "\n",
    "*SQLAlchemy:* ORM (Object Relational Mapping) más popular para interactuar con bases de datos SQL.\n",
    "  - Abstrae las diferencias entre diferentes sistemas de bases de datos\n",
    "  - Permite realizar consultas SQL complejas de manera sencilla y segura\n",
    "\n",
    "*ODBC:* El estándar en conexión a bases de datos.\n",
    "  - Opens Databases Connectivity es una interfaz universal para acceder a diferentes bases de datos\n",
    "  - SQLAlchemy utiliza ODBC para conectarse a diversas fuentes de datos, facilitando la integración en pipelines ETL.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cbab7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
